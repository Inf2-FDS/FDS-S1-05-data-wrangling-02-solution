{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science\n",
    "## S1 Week 05: Data wrangling II - groupby and regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning outcomes:** \n",
    "In this lab, we will build on what you have previously learned on preparing data for future use. By the end of the lab you should be able to:\n",
    "\n",
    "- use regular expressions to parse textual data\n",
    "- organise columns into indicator variables when appropriate\n",
    "- apply group-wise computations to your data\n",
    "\n",
    "In this week's lab, you will be analysing data from the course survey from last year, as well as data obtained from the department of Informatics timetabling. There are many instances whereby you might want to create a form and analyse user responses to it - let this be a lesson in how (not?) to design a form ü§î.\n",
    "\n",
    "**Data description:** The data from the first dataset for this lab originates from last year's course optional survey, which included several questions given to past students inquiring about their remote working setup, Python expertise and interests. The data was anonymised prior to being provided for this lab. The second dataset is the Informatics timetables which can be obtained from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Exploring the survey data\n",
    "\n",
    "### A.1 Loading and preparing the survey data\n",
    "\n",
    "**Exercise 01:** Load `survey_data.xlsx` into a variable `survey_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "survey_data_path = os.path.join(os.getcwd(), 'datasets', 'survey_data.xlsx')\n",
    "survey_data = pd.read_excel(survey_data_path)\n",
    "survey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever working with data, remember it's important to first get an idea of what your data looks like. Let's start by having a look at the dtypes of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of 'ID', pandas has inferred the types of the remaining columns as Python 'objects' (text or mixed numeric and non-numeric values). In particular, notice that we have a column named as though it should contain a timestamp, '...what is the time in my current location?'. We'll look at converting this to a proper time format.\n",
    "\n",
    "But first, you might (not!) have noticed the double space typo in the second column name. Let's tidy up some of the names so they're easier to reference later. Using [`pandas.DataFrame.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.htmlhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html), we can specify the names of existing columns and what they should be renamed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = survey_data.rename(columns={survey_data.columns[1]: 'Preferred tutorial loc', # This is equivalent to {'My  preference for tutorial location is': 'Preferred tutorial loc', ...\n",
    "                                          survey_data.columns[2]: 'Time at 10:00 BST',\n",
    "                                          survey_data.columns[3]: 'Python expertise',\n",
    "                                          survey_data.columns[4]: 'IT equipment',\n",
    "                                          survey_data.columns[5]: 'Interests'})\n",
    "print(\"Column names:\", list(survey_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although date and time are not a data type of their own in Python, the [Datetime](https://docs.python.org/3/library/datetime.html#module-datetime) module supplies classes and functions to work with them. Representing textual time columns as Datetime objects can be handy for querying rows that meet specific time conditions and performing simple date and time arithmetic. The pandas equivalent to the Python Datetime object is [Timestamp](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html).\n",
    "\n",
    "### A.2 Converting textual time data to a datetime representation\n",
    "\n",
    "One option is to use [`pd.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) to convert the string values in a column to a Timestamp if the string meets a format of our choice (specified using [strftime](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) date format codes, e.g. `%m/%d/%Y` to match with `12/19/1995`). This function has the pandas `errors` argument that we've seen before in lab 2 (in `pd.to_numeric`), allowing us to `raise` an error if a value doesn't match the desired format, `coerce` it to be set as `NaT` or `ignore` it to return it as it is.\n",
    "\n",
    "If the values in our time column don't follow a rigid string format already, we might end up losing a lot of our data.\n",
    "\n",
    "**Exercise 02:** Print out the _unique_ values in the 'Time at 10:00 BST' column and their corresponding count in the column.\n",
    "\n",
    "**Hint:** A single pandas method can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "survey_data['Time at 10:00 BST'].value_counts()  # If dropna=True, NaN values will also be counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 01:** Did you expect to see so many variations in the format of the values in the time column? Discuss with your partner the implications of this on using the function [`pd.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) to convert the column to a datetime representation. What [`strftime()`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) format could you use to successfully convert most of the values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "* There are approx. 12 different ways that the time values have been input to the form by users. These include differences in spacing, usage of am/pm, colons, full-stops, letter casing, and some values which have been inferred as NaN.\n",
    "* We could capture the majority of the values using the `strftime()` format '%H:%M'. This would leave us having to ignore over 50 user responses and convert to them NaN values. Including NaN values that might already exist in the column, this would mean losing around 15% of our data.\n",
    "* In many cases, this might be acceptable; however, capturing as much of the data as possible might be really important in some contexts. For example, where the data concerns people's finances or transactions, error codes from medical devices, or volatile chemical production control software (e.g. the Hinkley Point C Nuclear power plant is software controlled!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How else could we clean up more of the textual time column prior to attempting to convert it a useful datetime type?\n",
    "\n",
    "### Regular expressions\n",
    "\n",
    "You may have encountered [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) before, which are also referred to as __regexs__. These are sets of character combinations that Python (and many other programming languages) can understand and use as rules to search through character strings.\n",
    "\n",
    "You can use these regex rules to ask questions such as ‚ÄúDoes this string match the pattern?‚Äù, or ‚ÄúIs there a match for the pattern anywhere in this string?‚Äù. You can also use regexs to modify or split apart a string in various ways.\n",
    "\n",
    "We can use the Python [`re`](https://docs.python.org/3/library/re.html) package for this. Some simple examples are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: a pattern that matches with the first name Sarah or Sara\n",
    "pattern = '^Sara(h?)$'\n",
    "# ^ matches with the start\n",
    "# () contains an expression to capture and group\n",
    "# ? zero or one occurrence\n",
    "# $ matches with the end\n",
    "\n",
    "# re.search looks for any location where the regex matches; re.match checks only the beginning.\n",
    "# If successful, it returns a Match object with info such as where the match starts and ends,\n",
    "# the substring it matched, and more. If no match is found, None is returned.\n",
    "print(re.search(pattern, 'Sarah'))\n",
    "print(re.search(pattern, 'Sara'))\n",
    "print(re.search(pattern, 'sarah'))\n",
    "\n",
    "# re.findall returns all substrings where the regex matches as a list\n",
    "eg_sentence = 'Sarah went to the park. Sarah wished it would rain.'\n",
    "print(re.findall('Sara(h?)', eg_sentence))\n",
    "print(re.findall('Sara(?:h)', eg_sentence))\n",
    "# The last statement uses (?:...), a non-capturing version of regular parentheses.\n",
    "# It matches whatever regex is inside the parentheses, but the substring matched by the group cannot\n",
    "# be retrieved after performing a match or referenced later in the pattern (re.findall otherwise returns the matched group only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several [methods](https://docs.python.org/3/library/re.html#match-objects) can be called on the 'Match' object that's returned to us. One useful method is `group()` which returns one or more string subgroups of the match. By default it returns the entire match, but if we pass an integer (up to 100) the substring matching the corresponding parenthesised group is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excerise 03:** Can you consult the Python [regex syntax](https://docs.python.org/3/library/re.html) documentation to construct a general regular expression pattern to extract the string 'bulldog' from the string 'data/CIFAR-10/images/bulldog_146.jpg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_str = 'data/CIFAR-10/images/bulldog_146.png'\n",
    "# your code\n",
    "pattern = re.compile(r'([^/]+)_\\d+.png$')\n",
    "# .png$ makes sure we just have .png at the end of the line\n",
    "# \\d matches numeric digits and + means one or more of them\n",
    "# _ is the underscore that appears in the original string just before the numbers\n",
    "# ([^/]+) matches a group of characters that do not contain a forward slash:\n",
    "## [...] matches any single characters in backets and [^...] is negation (matches any single character not in brackets)\n",
    "## Note that ^ matches beginning of the line.\n",
    "\n",
    "match = pattern.search(search_str)\n",
    "print(\"Entire matching string:\", match.group(0))\n",
    "print(\"The match corresponding to the parenthesised group:\", match.group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, back to cleaning up the time data column. We can use regular expressions to define a set of rules that progressively capture the variations in the format of strings in the 'Time at 10:00 BST' column, and substitute the matched sub-strings appropriately to convert them to a consistent string format.\n",
    "\n",
    "**Exercise 04:** In the cell below, `time_clean()` is a stub of a function that cleans the times. We've done some of the more tricky replacements for you, and your task is to try making some of the other replacements. Because this could be a very time-consuming and frustrating exercise, we've given the solution in the file `functions.py`. If you want to move on and come back to the exercise, you can do so by uncommenting the `import` line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_clean(t):\n",
    "    \"\"\"Clean a string from the survey data to a consistent string format ('%H:%M').\n",
    "    This function should handle the string cases found in Exercise 02.\"\"\"\n",
    "    if isinstance(t, str):  # NaN values will be of type float\n",
    "        # re.sub replaces the parts of a string matching a pattern with a desired replacement\n",
    "        t = re.sub('Ôºö', ':', t) # Replace \"fullwidth colon\" (Unicode character U+FF1A) with standard colon\n",
    "        t = re.sub(' +', '', t) # Remove spaces in the middle of the string\n",
    "        if re.match('^([^:]+)[Pp][Mm]', t): # 5pm -> 17:00\n",
    "            hour = re.sub('^([^:]+)[Pp][Mm]', '\\\\1', t)\n",
    "            t = str((int(hour) + 12)) + \":00\"\n",
    "        # Your answer:    \n",
    "        # More lines in the format\n",
    "        # t = re.sub...\n",
    "        \n",
    "    \n",
    "    return(t)\n",
    "\n",
    "# Uncomment the following for a function that works\n",
    "from solutions import time_clean \n",
    "\n",
    "# Example string to clean\n",
    "example_time = \"10:00(10am) \"\n",
    "print(example_time, \"->\", time_clean(example_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 05:** Use `time_clean()` to convert each value in the 'Time at 10:00 BST' column to a pandas Timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "survey_data['Time at 10:00 BST'] = [pd.Timestamp(time_clean(t)) for t in survey_data['Time at 10:00 BST']]\n",
    "\n",
    "# Equivalent to:\n",
    "# survey_data['Time at 10:00 BST'] = survey_data['Time at 10:00 BST'].apply(lambda x: pd.Timestamp(timeclean(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our column is represented as a datetime, we can perform time-related queries on it such as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_midday = survey_data['Time at 10:00 BST'] >= \"12:00\"\n",
    "pm_df = survey_data.loc[after_midday]\n",
    "print(\"Number of users in the 'pm' during the lecture:\", pm_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 06:** Query the data to determine:\n",
    "\n",
    "a) the earliest time the lecture takes place for any of the students.\n",
    "\n",
    "b) the number of users that are local to UK time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "earliest_time = survey_data['Time at 10:00 BST'].min()\n",
    "print(\"Earliest time:\", earliest_time.time())\n",
    "\n",
    "local_count = (survey_data['Time at 10:00 BST'] == \"10:00\").sum()\n",
    "print(\"Number of users that are local to UK time:\", local_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Dummy / Indicator variables\n",
    "\n",
    "The survey data has several columns that contain categorical entries, and users can have multiple categorical options for 'IT equipment' and 'Interests' which are currently separated by semicolons. \n",
    "\n",
    "Depending on the analysis you might want to perform, it can be beneficial and neccessary to create dummy or indicator variables in order to handle numerical computations on your data. These are binary variables that indicates whether a separate categorical variable takes on a specific value for an entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pd.Series.str.get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get_dummies.html) is a function we can use to convert our categorical variable into dummy/indicator variables. We use dummy variables when applying regression and classification algorithms to categorical data. It also has a handy argument for specifying a separator on which to split our multiple entries by. After converting to this format, the presence of the category receives a value of 1 and in its absence the value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipment_expanded = survey_data['IT equipment'].str.get_dummies(sep=\";\")\n",
    "equipment_expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_equip_expanded = pd.concat([survey_data, equipment_expanded], axis=1)\n",
    "survey_equip_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 07 (optional):** You will see that some of the column headers are very long. This is because, along with a categorical list of 'IT equipment', users could also specify free-written responses to contain extra details. Write a regular expression that removes any sub-entry specified which is longer than three words and apply it to the 'IT equipment' column. Make sure to keep an eye out for NaN values and whitespace. Then repeat the conversion above to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "pattern = re.compile(r'(\\w+\\s){3,}\\w+\\s?;')\n",
    "def shorten_equip(equip):\n",
    "    if isinstance(equip, str):\n",
    "        short_equip = pattern.sub('', equip)\n",
    "        return short_equip.replace('; ', '')\n",
    "    return ''  # NaN values\n",
    "\n",
    "survey_data['IT equipment'] = survey_data['IT equipment'].apply(lambda x: shorten_equip(x))\n",
    "\n",
    "equipment_expanded = survey_data['IT equipment'].str.get_dummies(sep=\";\")\n",
    "survey_equip_expanded = pd.concat([survey_data, equipment_expanded], axis=1)\n",
    "survey_equip_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. School of Informatics timetabling\n",
    "\n",
    "### B.1 Loading the Informatics timetables\n",
    "\n",
    "The timetables for courses in the School of Informatics can be found online (e.g. see ours [here](https://www.ted.is.ed.ac.uk/UOE2021_SWS/TIMETABLE.ASP?OBJECTCLASS=MODULE&IDENTIFIER=INFR08030_SS1_YR&STYLE=TEXTSPREADSHEET&TEMPLATE=SWSCUST+OBJECT+TEXTSPREADSHEET&WEEK=9-37)). We downloaded the webpage specifying the timetables for all of the courses within the School of Informatics for Semester 1 in 2021-22; see `SWSCUST Object TextSpreadsheet.html` in the `datasets` folder. The timetables are written in Hypertext Markup Language (HTML), a language which defines the meaning and structure of web content. It's exactly what you would see if you looked at the page source in the browser.\n",
    "\n",
    "We will learn more about the details of web-scraping in a later lab. Today we will focus on using pandas to read a HTML file and performing group-wise computations across the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option for specifically parsing HTML tables is to use [`pd.read_html()`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html). This function searches for the `<table>` elements in the HTML page and returns them as a list. \n",
    "\n",
    "We advise opening 'SWSCUST Object TextSpreadsheet.html' in your web-browser to have a better look at it. You might notice that many of the tables are empty, under certain days of the week for example. We can use the `match` argument to only return tables that contain text matching a string or regular expression that we specify. The value defaults to `.+` (i.e. matches any non-empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable_data_path = os.path.join(os.getcwd(), 'datasets', 'SWSCUST Object TextSpreadsheet.html')\n",
    "timetable_data_list = pd.read_html(timetable_data_path, match='Activity', header=0)\n",
    "print(\"Tables parsed:\", len(timetable_data_list))\n",
    "print(\"Example table:\")\n",
    "timetable_data_list[2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 08:** We currently have a list of DataFrames, where each DataFrame is a table parsed from the HTML page. Can you create one DataFrame that contains all of these tables? Assign it the variable name `timetable_data`.\n",
    "\n",
    "- **Hint 1:** Printing the shape of the resulting DataFrame should return (501, 9).\n",
    "- **Hint 2:** You met the required function in the first Data Wranging lab, but there its argument was a list with only two members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "timetable_data = pd.concat(timetable_data_list, ignore_index=True)\n",
    "timetable_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can also be useful for stripping extra whitespace or unnecessary characters from your data.\n",
    "\n",
    "**Exercise 09:** Use a regular expression to strip the '*' characters from the values in the 'Type' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "timetable_data[\"Type\"] = timetable_data[\"Type\"].apply(lambda x: re.sub(\"\\*\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `pd.read_html` only parses `<table>` elements, with the layout of 'SWSCUST Object TextSpreadsheet.html' its tricky to neatly associate each table with its associated day of the week (which are specified in `<p>` tags) and course. HTML can be notoriously messy to parse. Thankfully, there are several external libraries which can be useful for this task, some of which we'll look at in a later lab.\n",
    "\n",
    "The current 'Activity' column of our data contains the name of the course, and sometimes specific details regarding the activity. For now, we'll try to remove the latter details so we can later group our timetables by course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def course_name_from_activity(activity_str):\n",
    "    \"\"\"Attempts to extract the course name from the 'Activity' column\n",
    "    of the timetable data. Warning: this function is not perfect! There are still some\n",
    "    exceptions in the formatting of the course names which are not captured.\"\"\"\n",
    "    if re.search('[-]', activity_str):\n",
    "        match = re.match('^(Informatics \\d - )?.*(?= - )', activity_str)\n",
    "        assert match is not None, f\"No match found for '{activity_str}'.\"\n",
    "        return match.group()  # Return the matched string (instead of Match object)\n",
    "    return activity_str\n",
    "\n",
    "timetable_data[\"Course Name\"] = timetable_data[\"Activity\"].apply(lambda x: course_name_from_activity(x))\n",
    "\n",
    "num_activities = timetable_data[\"Activity\"].nunique()\n",
    "extracted_courses = timetable_data[\"Course Name\"].nunique()\n",
    "print(f\"{num_activities} detailed activities reduced to {extracted_courses}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Groupwise computations\n",
    "\n",
    "By _groupby_ (or _split-apply-combine_) we are referring to a process involving one or more of the following steps:\n",
    "\n",
    "* Splitting the data into groups based on some criteria.\n",
    "* Applying a function to each group independently. This could include computing a summary statistic for each group, such as a mean or count; performing some group-specific computation such as standardising the data within a group; as well as filtering to discard data that has only a few members, for example.\n",
    "* Combining the results into a data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's drop some of the empty or redundant columns\n",
    "timetable_data = timetable_data.drop(['Activity', 'Description', 'Staff'], axis=1)\n",
    "timetable_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group our data according to course name, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by variable of interest\n",
    "grouped_by_name = timetable_data.groupby(\"Course Name\")\n",
    "# Retrieve a given group by name\n",
    "grouped_by_name.get_group(\"Accelerated Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations can then be performed on the grouped DataFrame like finding the size of each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of 'activities' scheduled for each course in semester 1\n",
    "grouped_by_name.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that a few of the groups have the size that we expect in the list above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grouped_by_name.get_group(\"Accelerated Natural Language Processing\")))\n",
    "print(len(grouped_by_name.get_group(\"Natural Computing\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each course, display the number of scheduled entries for each activity type\n",
    "timetable_data.groupby([\"Course Name\", \"Type\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply various functions to `GroupBy` objects. For example `.mean()` and `.median()` compute the mean and median of the numeric variables in each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Ignoring entries which have NaN values, perform group-wise computations on the timetable data (and refer to pandas documentation) to answer the following questions:\n",
    "\n",
    "a) Which building has the largest number of rooms in it? \n",
    "\n",
    "b) Which room is used the most across all the buildings, and which building is it in?\n",
    "\n",
    "c) What is the mean and median length of a 'Lecture' in Informatics?\n",
    "\n",
    "d) Which on average, is the longest activity type scheduled (i.e. lectures, tutorials, labs, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# Building with most rooms\n",
    "room_count = timetable_data.groupby([\"Building\"])[\"Room\"].nunique()\n",
    "print(room_count)\n",
    "print(room_count.idxmax(), \"has\", room_count.max(), \"rooms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# Most used room across the buildings\n",
    "most_used = timetable_data.groupby([\"Building\", \"Room\"]).size().idxmax()\n",
    "print(most_used[1], \"in\", most_used[0], \"is the most used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# Set errors arg to 'raise' so we're aware of data that doesn't conforms\n",
    "# Note that by default, pd.to_datetime initialise the datetime objects from the UNIX 'beginning of time'\n",
    "timetable_data['Start'] = pd.to_datetime(timetable_data['Start'], errors='raise', format='%H:%M')\n",
    "timetable_data['End'] = pd.to_datetime(timetable_data['End'], errors='raise', format='%H:%M')\n",
    "\n",
    "grouped_by_type = timetable_data.groupby('Type')\n",
    "activity_durations = grouped_by_type.apply(lambda grp: grp['End'] - grp['Start'])\n",
    "\n",
    "# Your code\n",
    "# Mean length of a Lecture\n",
    "print(\"Mean duration of a lecture:\", activity_durations[\"Lecture\"].mean())\n",
    "print(\"And median duration:\", activity_durations[\"Lecture\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# Longest activity type scheduled\n",
    "longest_session_per_type = activity_durations.groupby(\"Type\").max()\n",
    "average_durations = activity_durations.groupby(\"Type\").mean()\n",
    "\n",
    "print(\"Longest session duration for each type:\\n\", longest_session_per_type, \"\\n\")\n",
    "print(\"Average session duration for each type:\\n\", average_durations, \"\\n\")\n",
    "print(\"Longest activity scheduled on average:\", average_durations.idxmax(), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "49bd69289ce3351ea88ada7e23166e5bac6edb151d8bda4b1b5987655ded3d35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
